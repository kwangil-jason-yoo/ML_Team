{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Sentiment Analysis\n",
    "\n",
    "A tweet or a review can often tell us how its author feels about the subject of the text. In this homework, we will use the [IMDB review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) to automatically infer for a given review whether it's positive, hence its author liked the movie, or negative. This subtype of text classification which aims to infer the sentiments of the text's author is **sentiment analysis**.\n",
    "\n",
    "Naive Bayes is a probabilistic classifier which assigns among all classes the class with highest probability to a given instance. In our case, we will use it to classify movie review texts into a positive and a negative class. For a given text or document $d$ Naive Bayes decides on the class $\\hat{c}$ among all classes $C$ according to the formula:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{c} &= \\underset{c\\in C}{\\operatorname{argmax}} P(c \\mid d)\\\\\n",
    "&=\\underset{c\\in C}{\\operatorname{argmax}}\\frac{P(d \\mid c)P(c)}{P(d)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "$P(d)$ is the same for all classes and will not affect which class is more probable, we can disregard it. We will need to calculate the other two quantities likelihood $P(d \\mid c)$ and class prior probabilities $P(c)$. \n",
    "\n",
    "A movie review $d$ can be represented by features $w_i$ where $i$ indicates word position. Hence, we can rewrite our likelihood for document $d$ with $n$ words as\n",
    "\n",
    "\\begin{align}\n",
    "P(d \\mid c) = P(w_1, w_2, w_3,\\dots, w_n \\mid c)\n",
    "\\end{align}\n",
    "\n",
    "We are using the **bag-of-words** assumption. That is, we only care about the words themselves but not their positions. A model with this assumption would evaluate \"Apple likes you.\" and \"You like apples.\" the same after some common text preprocessing steps. Also, we will make the naive Bayes assumption which simplifies the calculation of the likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "P(d \\mid c) &= P(w_1, w_2, w_3,\\dots, w_n \\mid c)\\\\ \n",
    "&=P(w_1 \\mid c)P(w_2 \\mid c) \\dots P(w_n \\mid c)\n",
    "\\end{align}\n",
    "\n",
    "We will determine the likelihood of the word in a given position $i$ using its count (bag-of-words model). Using the naive Bayes assumption, we will count the occurence of single words (**unigrams**), e.g. \"apples\". **Bigram**, e.g. \"like apples\", **trigram** or higher order models violate the naive Bayes assumption. The likelihood will be the fraction of all words in class $c$ that are the same as word $w_i$.\n",
    "\n",
    "**Note**: For a given word $w_i$ = \"apple\", $P(w_i = \\text{\"apple\"} \\mid c)$ is calculated by counting the occurence of \"apple\" in **all** documents in class $c$. \n",
    "\n",
    "Our final class decision will be made by:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{c} &= \\underset{c\\in C}{\\operatorname{argmax}}P(c)\\underset{0<i\\leq n}{\\operatorname{\\Pi}}P(w_i \\mid c)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections of The Notebook\n",
    "\n",
    "1. [Reading Training and Test Sets](#data)<br>\n",
    "    1.1 [Summary of Dataset](#dataset-summary)<br>\n",
    "    1.2 [An Example Review](#example-review)<br>\n",
    "2. [Classification with scikit-learn](#sklearn)<br>\n",
    "    2.1 [CountVectorizer](#count-vectorizer)<br>\n",
    "    2.2 [MultinomialNB](#multinomialNB) <br>\n",
    "    2.3 [Sklearn Pipeline](#sklearn-pipeline)<br>\n",
    "3. [Processing with SpaCy Language Model](#spacy)<br>\n",
    "4. [Sklearn with SpaCy Lemmatization and Vocabulary](#sklearn-with-spacy)\n",
    "5. [Multinomial Naive Bayes Implementation](#multinomialnb-implementation)\n",
    "6. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 1. Reading Traning and Test Sets\n",
    "\n",
    "We will read in the training and testing examples and labels. The examples in this dataset are review texts with some html occasionally. The label information will be deduced from the folder name. Read more about the dataset and folder organization in [README](#dataset-summary).\n",
    "\n",
    "**Please be aware that the data format and organization will be somewhat different from dataset to dataset. Running the code below as it is will not work on most other datasets.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data && curl https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz > data/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tarfile is needed to read from a tar archive\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path specification for the files\n",
    "ARCHIVE_NAME = \"./data/aclImdb_v1.tar.gz\"\n",
    "README_NAME = \"aclImdb/README\"\n",
    "TRAIN_FOLDER = \"train/\"\n",
    "TEST_FOLDER = \"test/\"\n",
    "POSITIVE_FOLDER = \"pos/\"\n",
    "NEGATIVE_FOLDER = \"neg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions for reading data from archive and\n",
    "# populating training and test sets\n",
    "\n",
    "\n",
    "def text_decoder(text):\n",
    "    \"\"\"Decodes to utf-8\"\"\"\n",
    "    return text.decode()\n",
    "\n",
    "\n",
    "def truncate_to(text, pos=1000):\n",
    "    \"\"\"Show text until the character at pos index\"\"\"\n",
    "    return text[:pos]\n",
    "\n",
    "\n",
    "def archive_file_contents(tar, info):\n",
    "    \"\"\"Get contents of tar file\"\"\"\n",
    "    f = tar.extractfile(info)\n",
    "    return f.read()\n",
    "\n",
    "\n",
    "def get_raw_data_from(tar):\n",
    "    \"\"\"Reads in the tar archive, forms the training and test set\"\"\"\n",
    "\n",
    "    train_reviews = []\n",
    "    train_labels = []\n",
    "    test_reviews = []\n",
    "    test_labels = []\n",
    "\n",
    "    # for each file in the archive,\n",
    "    # get the filename and tarinfo of the compressed file\n",
    "    for fname, farchive in zip(tar.getnames(), tar.getmembers()):\n",
    "        # if the file is in the training set\n",
    "        if TRAIN_FOLDER in fname:\n",
    "            # and a positive review\n",
    "            if POSITIVE_FOLDER in fname:\n",
    "                # add the review to the train_reviews list\n",
    "                train_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 1 to the train_labels list indicating the example is a positive review\n",
    "                train_labels.append(1)\n",
    "\n",
    "            # if the file is a negative review\n",
    "            if NEGATIVE_FOLDER in fname:\n",
    "                # add the review to the train_reviews list\n",
    "                train_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 0 to the train_labels list indicating the example is a negative review\n",
    "                train_labels.append(0)\n",
    "\n",
    "        # if the file is in the test set\n",
    "        elif TEST_FOLDER in fname:\n",
    "            # and a positive review\n",
    "            if POSITIVE_FOLDER in fname:\n",
    "                # add the review to the test_reviews list\n",
    "                test_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 1 to the test_labels list indicating the example is a positive review\n",
    "                test_labels.append(1)\n",
    "\n",
    "            # if the file is a negative review\n",
    "            if NEGATIVE_FOLDER in fname:\n",
    "                # add the review to the test_reviews list\n",
    "                test_reviews.append(text_decoder(archive_file_contents(tar, farchive)))\n",
    "                # add a 0 to the test_labels list indicating the example is a negative review\n",
    "                test_labels.append(0)\n",
    "\n",
    "    return train_reviews, train_labels, test_reviews, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(ARCHIVE_NAME, \"r:gz\") as tar:\n",
    "    X_train, y_train, X_test, y_test = get_raw_data_from(tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset-summary\"></a>\n",
    "### 1.1 Summary of Dataset\n",
    "\n",
    "Information about the dataset from the included README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(ARCHIVE_NAME, \"r:gz\") as tar:\n",
    "    readme = text_decoder(archive_file_contents(tar, README_NAME))\n",
    "    # only the first 1761 characters are informative for us\n",
    "    print(truncate_to(readme, 1761))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-review\"></a>\n",
    "### 1.2 An Example Review\n",
    "\n",
    "To have an idea of what the reviews look like, we can print the contents of a random review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# setting a seed makes sure we get consistent results across runs\n",
    "random.seed(142)\n",
    "ind = random.randint(a=0, b=len(X_train))\n",
    "review_example = X_train[ind]\n",
    "print(review_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn\"></a>\n",
    "## 2. Classification with sklearn\n",
    "\n",
    "Scikit-learn ([sklearn](https://scikit-learn.org/stable/)) is a powerful library that simplifies machine learning. Most of the well-known machine learning algorithms are implemented as well as methods for hyperparameter selection, model evaluation, etc. We will use the feature extraction module to calculate word counts, metrics module to calculate accuracies, naive_bayes module for the classification algorithm, and finally pipeline module to put parts of our model together to run more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"count-vectorizer\"></a>\n",
    "### 2.1 CountVectorizer\n",
    "\n",
    "CountVectorizer returns the number of occurrences of words in the documents. It can take several arguments, which are listed in [its documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). In this section, we will use it with its default arguments. Later, we will specify the vocabulary for which the counts will be calculated, and some other parameters. When the vocabulary is not explicitly specified, it's constructed from the union of all words in all documents.\n",
    "\n",
    "The shape of the returned counts will be $n_{\\text{docs}} \\times n_{\\text{vocab}}$, where $n_{\\text{docs}}$ is the number of documents (in this case just the `review_example`, hence 1), and $n_{\\text{vocab}}$ is the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "example_counts = vectorizer.fit_transform(\n",
    "    [review_example]\n",
    ")  # CountVectorizer object expects an iterable such as a list\n",
    "\n",
    "# CountVectorizer returns a sparse representation.\n",
    "# To see its contents, we will need to cast it\n",
    "# to a dense array.\n",
    "example_counts_dense = example_counts.toarray()\n",
    "\n",
    "# vocabulary is a one-to-one mapping between words and integers\n",
    "# Each of these integers specify the index of the corresponding word in the count array returned\n",
    "\n",
    "list(vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(list(vectorizer.vocabulary_.items()))\n",
    "print(f\"There are {n_vocab} distinct words in the review_example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check which words were the most frequent in `review_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the most frequent word and how many times it occurs in the example review\n",
    "reverse_vocabulary_lookup = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "len(list(reverse_vocabulary_lookup.items()))\n",
    "\n",
    "print(\n",
    "    f\"The most frequent word is '{reverse_vocabulary_lookup[np.argmax(example_counts_dense)]}' with {np.max(example_counts_dense)} occurences.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we gaining any information from the most frequent word? If you change the seed for the random number generator or the index into the training set in [example review section](#example-review), you will see that the most frequent words in the documents are words like \"the\", \"and\" which are not informative. These words are called **stop words**, we will clean the documents from stop words in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=multinomialNB></a>\n",
    "### 2.2 MultinomialNB\n",
    "\n",
    "For our problem, we can use the MultinomialNB (multinomial naive Bayes) implemented in `sklearn`. First, we will need to fit and transform our training data with the `CountVectorizer` instance. Then, we will fit the MultinomialNB classifier to our training dataset, i.e., we will train our MultinomialNB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = vectorizer.fit_transform(X_train)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts = vectorizer.fit_transform(X_test)\n",
    "\n",
    "# If you run the two lines of code below you will get a dimension mismatch error.\n",
    "# See the text before Section 1.3.\n",
    "\n",
    "# test_predictions = classifier.predict(test_counts)\n",
    "# accuracy_score(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_examples, n_train_corpus = train_counts.shape\n",
    "n_test_examples, n_test_corpus = test_counts.shape\n",
    "print(\n",
    "    f\"Training set has {n_train_examples} reviews with a vocabulary of size {n_train_corpus}.\"\n",
    ")\n",
    "print(\n",
    "    f\"Test set has {n_train_examples} reviews with a vocabulary of size {n_test_corpus}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different vocabularies in the training and test sets lead to \"dimension mismatch error\". We can fix a vocabulary beforehand and disregard words not in that vocabulary in all of the computations. Instead of specifying the words in the vocabulary explicitly, we will use **SpaCy**'s English model vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn-pipeline\"></a>\n",
    "### 2.3 Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn allows us to define a custom pipeline where each component of the pipeline can be custom. The inputs to the components are the outputs of the previous component in the pipeline. The code below is equivalent to classification steps in the previous [CountVectorizer](#count-vectorizer) and [MultinomialNB](#multinomialNB) sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"vectorize\", CountVectorizer()), (\"clf\", MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict the test set classes and report model accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(f\"Test accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"spacy\"></a>\n",
    "## 3. Processing with SpaCy Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is a Natural Language Processing library in Python. It offers a lot of functionality. We will use its lemmatization method in this notebook for preprocessing the reviews.\n",
    "\n",
    "We will need to load the English language model. It is in the `shared` folder of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import the SpaCy library\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm-2.3.1\", disable=[\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the similarities between words that come from the same root, such as \"apple\" vs \"apples\" and \"bring\" vs \"brought\", we can either stem or lemmatize the words. Stemming doesn't always result in actual words, for example \"brought\" might end up as \"brough\". Lemmatization produces whole actual words. We will see examples of lemmatization below, and later use lemmatization in our sklearn pipeline. You can read more about [SpaCy Tokens](https://spacy.io/api/token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy models come with taggers, parsers and named-entity-recognizers.\n",
    "# We will disable the parts of the spacy processing pipeline that we will not need.\n",
    "# doc will be a series of SpaCy Tokens\n",
    "\n",
    "doc = nlp(review_example, disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "# You can think about a SpaCy token\n",
    "# as a word in the doc with extra information\n",
    "# coming from SpaCy processing\n",
    "# like position in document, lemma, etc.\n",
    "\n",
    "# For each token in the document\n",
    "for token in doc:\n",
    "    # if the token is not a stop word or a punctuation mark\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        # print the token text and its lemmatization\n",
    "        print(f\"{token.text:<30}{token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=sklearn-with-spacy></a>\n",
    "## 4. Sklearn with SpaCy Lemmatization and Vocabulary\n",
    "\n",
    "Finally, we can process the reviews with SpaCy lemmatization and vocabulary in the CountVectorizer. Then, run our MultinomialNB classifier on the output in a sklearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\"Cleans up html tags\"\"\"\n",
    "    html_regex = \"<[^>]*>*<[^>]*>\"\n",
    "    if type(text) == str:\n",
    "        text = re.sub(html_regex, \"\", text)\n",
    "        text = re.sub(\"[\\W]+\", \"\", text.lower())\n",
    "    return text\n",
    "\n",
    "\n",
    "# tokenize the doc, remove stop words and lemmatize its tokens\n",
    "def tokenize(doc):\n",
    "    tokens = nlp(doc)\n",
    "    return [preprocessor(token.lemma_) for token in tokens if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline below will take significantly longer than the above pipeline where we had kept the default settings for the CountVectorizer. SpaCy can be optimized to run faster with nlp.pipe and processing all of the documents before inputting to CountVectorizer. We will not cover nlp.pipe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                vocabulary=nlp.vocab.strings, tokenizer=tokenize, ngram_range=(1, 1)\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "spacy_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = spacy_pipeline.predict(X_test)\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer also has some processing steps such as converting to lowercase, which is set to True by default, and removing stop words. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=multinomialnb-implementation></a>\n",
    "## 5. Multinomial Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: a sparse matrix of counts\n",
    "            y: Labels array\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # In the fit method we'll estimate the parameters of the model.\n",
    "        # In sklearn, fit method in any class extended from BaseEstimator should\n",
    "        # return the object itself.\n",
    "\n",
    "        # In our pipeline, X will be the output of the CountVectorizer instance\n",
    "        # and it is a sparse matrix.\n",
    "\n",
    "        # We want to take out only the non-zero entries\n",
    "        # and their coordinates (in document_number, word_index form)\n",
    "\n",
    "        counts_positive_ = defaultdict(int)\n",
    "        counts_negative_ = defaultdict(int)\n",
    "\n",
    "        # counts_ will be a dictionary of dictionaries\n",
    "        # It has two keys: 0 and 1\n",
    "        # 0 will contain the counts of words in the negative documents\n",
    "        # Similarly, 1 will contain the counts of words in the positive documents\n",
    "\n",
    "        counts_ = {0: counts_negative_, 1: counts_positive_}\n",
    "        class_counts_ = defaultdict(int)\n",
    "\n",
    "        # dictionary that keeps the total number of words in all documents in each class\n",
    "        total_word_counts = {0: 0, 1: 0}\n",
    "\n",
    "        # estimate parameters of the model (P(w_i|c))\n",
    "        # for each word w\n",
    "        for d, w in zip(*X.nonzero()):\n",
    "            # collect counts of w in every document\n",
    "            w_count_in_d = X[d, w]\n",
    "\n",
    "            # update the dictionary entries with the counts\n",
    "            # from X.\n",
    "            counts_[y[d]][w] += w_count_in_d\n",
    "\n",
    "            # We also need the total word counts in both the positive and negative groups\n",
    "            # Here, we are incrementing the total word count for the correct group.\n",
    "            total_word_counts[y[d]] += w_count_in_d\n",
    "\n",
    "        # the prior class probabilities\n",
    "        positive_fraction = sum(y) / len(y)\n",
    "        negative_fraction = 1 - positive_fraction\n",
    "\n",
    "        # log class priors\n",
    "        # max operation is needed to avoid invalid input to log\n",
    "        self.class_priors_ = {\n",
    "            0: np.log(negative_fraction),\n",
    "            1: np.log(positive_fraction),\n",
    "        }\n",
    "\n",
    "        # We will calculate the class conditional probability (likelihood)\n",
    "        # of each word.\n",
    "        all_words = set(list(counts_[0].keys()) + list(counts_[1].keys()))\n",
    "\n",
    "        # We initialize these likelihoods to 0\n",
    "        self.word_probs_ = defaultdict(lambda: {0: 0, 1: 0})\n",
    "\n",
    "        # then for each word, we'll update the word likelihoods\n",
    "        # the likelihood for a word will be the count of that word in all documents in class c\n",
    "        # divided by the count of all words in all documents in class c.\n",
    "        for v in all_words:\n",
    "            self.word_probs_[v][0] = np.log(max(1, counts_[0][v])) - np.log(\n",
    "                total_word_counts[0]\n",
    "            )\n",
    "            self.word_probs_[v][1] = np.log(max(1, counts_[1][v])) - np.log(\n",
    "                total_word_counts[1]\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: a sparse matrix of counts\n",
    "            y: None (kept for compatibility)\n",
    "        Returns:\n",
    "            preds: an array of predicted classes\n",
    "        \"\"\"\n",
    "\n",
    "        # For each document d in X,\n",
    "        # we will use the class priors and likelihoods calculated in fit\n",
    "        # to predict the predicted class for d.\n",
    "\n",
    "        d = 0\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # unnormalized posterior probabilites\n",
    "        prob_pos = self.class_priors_[1]\n",
    "        prob_neg = self.class_priors_[0]\n",
    "\n",
    "        prev_d = d\n",
    "\n",
    "        # The sparse matrix structure of CountVectorizer output X\n",
    "        # allows us to iterate over documents and words in X.\n",
    "\n",
    "        for d, w in zip(*X.nonzero()):\n",
    "\n",
    "            # When we encounter the next document in X,\n",
    "            # append the class prediction for the previous document to preds\n",
    "            # and reset the unnormalized posterior probability to the class prior.\n",
    "            if d != prev_d:\n",
    "                preds.append(1 if prob_pos > prob_neg else 0)\n",
    "                prob_pos = self.class_priors_[1]\n",
    "                prob_neg = self.class_priors_[0]\n",
    "                prev_d = d\n",
    "\n",
    "            # While we are still processing the same document,\n",
    "            # add the word likelihoods to the unnormalized posterior probabilities.\n",
    "            prob_pos += self.word_probs_[w][1]\n",
    "            prob_neg += self.word_probs_[w][0]\n",
    "\n",
    "        # append the class prediction for the last document\n",
    "        preds.append(1 if prob_pos > prob_neg else 0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"vectorizer\",\n",
    "            CountVectorizer(\n",
    "                vocabulary=nlp.vocab.strings, tokenizer=tokenize, ngram_range=(1, 1)\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", NaiveBayes()),\n",
    "    ]\n",
    ")\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=exercises></a>\n",
    "## 6. Exercises\n",
    "\n",
    "Another type of naive Bayes model known as **binary multinomial naive Bayes** (or binary naive Bayes) is commonly used for sentiment analysis. In this model, the likelihood $P(w_i \\mid c)$ is given by the fraction of documents in class $c$ that contain $w_i$ rather than the fraction of all words in class $c$ that are the same as $w_i$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Adapt the classifier defined in [Multinomial naive Bayes implementation](#multinomialnb-implementation) to include add-one (Laplace) smoothing and plug into the pipeline instead of the MultinomialNB classifier. Report accuracy on the test set.  You can implement your own classifier from scratch instead of adapting the above.\n",
    "\n",
    "**Hint:** Output of CountVectorizer instance has shape $n_{\\text{doc}} \\times n_{\\text{vocab}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove duplicates from each review (`doc`). Remember `doc` variable is an object of type `spacy.Tokens.Doc`.  It is a sequence of `Token` objects. A token in SpaCy is more than a simple text and tokens for the same text are not equal to each other. You will need to compare either the `text` or better yet the `lemma_` attributes of the tokens.\n",
    "\n",
    "#### After removing the duplicates, use the above pipeline (`spacy_pipeline`) to build a binary multinomial naive Bayes model. Report accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the tokenize function to remove duplicates from each review (doc)\n",
    "# it should return a list of lemmas as in the previous definition.\n",
    "# A better way would be to define a preprocessor and pass it to the preprocessor\n",
    "# parameter of CountVectorizer. To keep things simpler we will not be doing that\n",
    "# here.\n",
    "\n",
    "def tokenize(doc):\n",
    "    # \n",
    "    pass\n",
    "    \n",
    "# Reconstruct the pipeline. \n",
    "\n",
    "spacy_pipeline = ...\n",
    "\n",
    "# Fit the training set, make predictions for the test set and report accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "Please contact Zeynep Hakguder (<a href=\"mailto:zphakguder@gmail.com\">zphakguder@gmail.com</a>) for further questions or inquries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
